# Cost-Optimized Multi-Agent Implementation Plan

## Executive Summary

Implement Claude's multi-agent research architecture with **aggressive cost optimization** to keep costs at **$0.05-0.30 per query** (instead of $0.50-5.00).

## Cost Optimization Strategy

### 1. Model Routing (Primary Strategy)

**Lead Agent (Orchestrator) - Claude Opus 4**
- Used for: Planning, synthesis, high-level reasoning
- Calls per query: 1-2
- Cost: $15 per 1M input tokens
- Justification: Complex reasoning requires best model

**Worker Agents (Subagents) - Claude Sonnet 4**
- Used for: Execution, analysis, information gathering
- Calls per query: 3-5 parallel
- Cost: $3 per 1M input tokens (5Ã— cheaper)
- Justification: Focused tasks don't need Opus-level reasoning

**Cost Impact**: 40-60% reduction compared to all-Opus architecture

### 2. Prompt Caching (Already Implemented)

```python
# config/settings.py
ENABLE_PROMPT_CACHING = True  # 60-70% savings

# In comprehensive_analyzer.py
system_message = [{
    "type": "text",
    "text": system_prompt,
    "cache_control": {"type": "ephemeral"}  # Cache system prompts
}]
```

**What to cache:**
- System prompts (constant across queries)
- Document context (reused for multiple questions)
- RAG retrieved chunks (same chunks accessed multiple times)

**Cost Impact**: 60-70% reduction on cached content

### 3. Batch API Mode (Non-Urgent Queries)

```python
# config/settings.py
ENABLE_BATCH_MODE = True  # 50% cost savings for async processing

# New implementation needed:
class BatchProcessor:
    def submit_batch_query(self, query: str, research_plan: Dict):
        """Submit non-urgent research to Batch API (24h delivery, 50% discount)"""
        batch_request = self.client.messages.batches.create(
            requests=[{
                "custom_id": f"research_{timestamp}",
                "params": {
                    "model": "claude-sonnet-4",
                    "max_tokens": 8000,
                    "messages": messages
                }
            }]
        )
        return batch_request.id

    def check_batch_status(self, batch_id: str):
        """Poll batch status every 60 seconds"""
        batch = self.client.messages.batches.retrieve(batch_id)
        return batch.processing_status
```

**Use cases:**
- Literature reviews (not time-sensitive)
- Bulk document analysis
- Background research
- Weekly digest generation

**Cost Impact**: 50% reduction for batched queries

### 4. Intelligent Token Budgeting

**Query Classification (free local model):**
```python
class QueryClassifier:
    """Classify query complexity to determine token budget"""

    def classify(self, query: str) -> str:
        # Use free local model (Llama 3.1) for classification
        complexity = self.local_model.classify_complexity(query)

        if complexity == "simple":
            return "quick"  # 2-5K tokens, 1-2 agents
        elif complexity == "moderate":
            return "standard"  # 10-20K tokens, 2-3 agents
        else:
            return "deep"  # 30-75K tokens, 3-5 agents
```

**Token Allocation by Complexity:**
- **Simple queries** (fact lookup): 2-5K tokens, $0.01-0.05
- **Standard queries** (analysis): 10-20K tokens, $0.05-0.15
- **Deep research** (comprehensive): 30-75K tokens, $0.15-0.50

### 5. Hybrid Local + Cloud Architecture

**Use Local LLM (Free) for:**
- Query classification
- Chunk relevance scoring
- Citation extraction
- Text preprocessing
- Basic Q&A (single document)

**Use Claude API (Paid) for:**
- Multi-document synthesis
- Deep reasoning
- Cross-reference analysis
- Final report generation

```python
class HybridProcessor:
    def __init__(self):
        self.local_model = LocalLLMHandler()  # Free Llama 3.1
        self.cloud_model = ComprehensiveAnalyzer()  # Paid Claude

    def process_query(self, query: str):
        # Step 1: Local model classifies (FREE)
        complexity = self.local_model.classify(query)

        if complexity == "simple":
            # Step 2a: Local model answers directly (FREE)
            return self.local_model.answer(query)
        else:
            # Step 2b: Cloud model for complex reasoning (PAID)
            return self.cloud_model.research(query)
```

### 6. Retrieval Optimization (Reduce Redundancy)

**Current issue:** Sending too many redundant chunks to API

**Solution: Smart chunk deduplication**
```python
class SmartRetriever:
    def retrieve_optimized(self, query: str):
        # Step 1: Retrieve top-20 candidates (hybrid search)
        candidates = self.hybrid_search(query, top_k=20)

        # Step 2: Rerank to top-5 (local cross-encoder, FREE)
        reranked = self.local_reranker.rerank(candidates, top_k=5)

        # Step 3: Remove near-duplicates (FREE)
        deduplicated = self.deduplicate(reranked, threshold=0.85)

        # Step 4: Send only unique, relevant chunks to API (PAID)
        return deduplicated
```

**Cost Impact**: Send 3-5 unique chunks instead of 10 redundant chunks = 50% reduction

### 7. Progressive Research Strategy

**Instead of:** Full 5-agent research for every query

**Do this:** Progressive escalation
```python
class ProgressiveResearcher:
    def research(self, query: str):
        # Phase 1: Quick pass with 1 agent (2K tokens)
        quick_result = self.single_agent_search(query)

        # Check if user is satisfied
        if self.is_sufficient(quick_result):
            return quick_result  # Cost: $0.01

        # Phase 2: Medium depth with 2 agents (10K tokens)
        medium_result = self.dual_agent_search(query)

        if self.is_sufficient(medium_result):
            return medium_result  # Cost: $0.05

        # Phase 3: Deep research with 5 agents (50K tokens)
        deep_result = self.full_multi_agent_search(query)
        return deep_result  # Cost: $0.30
```

## Implementation Plan with Cost Focus

### Phase 1: Foundation + Model Routing (Week 1-2)
**Goal:** Implement multi-agent with intelligent routing

```python
# src/model_router.py (NEW FILE)
class ModelRouter:
    """Route tasks to appropriate model based on complexity"""

    def __init__(self):
        self.opus_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        self.sonnet_model = "claude-sonnet-4-5-20250929"
        self.opus_model = "claude-opus-4"

    def route_task(self, task_type: str, messages: List[Dict], system_prompt: str):
        """Route to Opus or Sonnet based on task"""

        # High-value tasks â†’ Opus
        if task_type in ["planning", "synthesis", "verification"]:
            model = self.opus_model
            logger.info(f"ðŸ§  Routing to Opus 4 for {task_type}")

        # Execution tasks â†’ Sonnet
        else:
            model = self.sonnet_model
            logger.info(f"âš¡ Routing to Sonnet 4 for {task_type}")

        response = self.opus_client.messages.create(
            model=model,
            max_tokens=8000,
            system=system_prompt,
            messages=messages
        )

        return response.content[0].text
```

**Cost Impact:** 40-60% reduction immediately

### Phase 2: Multi-Agent Architecture (Week 3-4)
**Goal:** Implement Lead Agent + Worker Agents with Sonnet

```python
# src/multi_agent_system.py (NEW FILE)
class LeadAgent:
    """Orchestrator using Opus 4 for planning"""

    def __init__(self, router: ModelRouter):
        self.router = router

    def plan_research(self, query: str) -> ResearchPlan:
        """Create research plan using Opus with extended thinking"""

        messages = [{
            "role": "user",
            "content": f"Plan research strategy for: {query}"
        }]

        # Use Opus for high-level planning
        plan_text = self.router.route_task(
            task_type="planning",
            messages=messages,
            system_prompt=PLANNING_SYSTEM_PROMPT
        )

        return self.parse_plan(plan_text)


class WorkerAgent:
    """Execution agent using Sonnet 4"""

    def __init__(self, agent_id: int, router: ModelRouter):
        self.agent_id = agent_id
        self.router = router

    async def execute_subtask(self, subtask: Dict, rag_system) -> Dict:
        """Execute research subtask using Sonnet (5Ã— cheaper)"""

        # Retrieve relevant context
        context, metadata = rag_system.get_relevant_context(
            subtask['query'],
            max_chunks=5
        )

        messages = [{
            "role": "user",
            "content": f"Research subtask: {subtask['query']}\n\nContext:\n{context}"
        }]

        # Use Sonnet for execution (cheaper)
        result = self.router.route_task(
            task_type="execution",
            messages=messages,
            system_prompt=EXECUTION_SYSTEM_PROMPT
        )

        return {
            "agent_id": self.agent_id,
            "subtask": subtask,
            "findings": result,
            "sources": metadata
        }


class MultiAgentOrchestrator:
    """Coordinates Lead Agent + Worker Agents"""

    def __init__(self):
        self.router = ModelRouter()
        self.lead_agent = LeadAgent(self.router)
        self.worker_agents = [WorkerAgent(i, self.router) for i in range(5)]

    async def research(self, query: str, rag_system) -> Dict:
        """Execute multi-agent research with cost optimization"""

        # Step 1: Lead agent plans (Opus, expensive but only 1 call)
        logger.info("ðŸ§  Lead Agent planning research strategy...")
        plan = self.lead_agent.plan_research(query)

        # Step 2: Assign subtasks to worker agents (Sonnet, cheap)
        logger.info(f"âš¡ Spawning {len(plan.subtasks)} worker agents...")
        tasks = [
            self.worker_agents[i].execute_subtask(subtask, rag_system)
            for i, subtask in enumerate(plan.subtasks)
        ]

        # Execute in parallel (concurrent API calls)
        results = await asyncio.gather(*tasks)

        # Step 3: Lead agent synthesizes (Opus, expensive but only 1 call)
        logger.info("ðŸ§  Lead Agent synthesizing findings...")
        final_report = self.lead_agent.synthesize_findings(results)

        return final_report
```

### Phase 3: Batch Mode Integration (Week 5)
**Goal:** Add batch processing for non-urgent queries

```python
# src/batch_processor.py (NEW FILE)
class BatchProcessor:
    """Handle batch API requests for 50% cost savings"""

    def __init__(self):
        self.client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

    def submit_batch_research(self, queries: List[str]) -> str:
        """Submit batch of research queries (24h delivery, 50% discount)"""

        requests = []
        for i, query in enumerate(queries):
            requests.append({
                "custom_id": f"research_{i}_{int(time.time())}",
                "params": {
                    "model": "claude-sonnet-4",
                    "max_tokens": 8000,
                    "messages": [{
                        "role": "user",
                        "content": f"Research query: {query}"
                    }]
                }
            })

        batch = self.client.messages.batches.create(requests=requests)
        logger.info(f"ðŸ“¦ Submitted batch {batch.id} with {len(queries)} queries")
        return batch.id

    def get_batch_results(self, batch_id: str) -> List[Dict]:
        """Retrieve completed batch results"""

        batch = self.client.messages.batches.retrieve(batch_id)

        if batch.processing_status != "ended":
            logger.info(f"â³ Batch still processing: {batch.processing_status}")
            return None

        # Download results
        results = []
        for result in batch.results:
            results.append({
                "custom_id": result.custom_id,
                "response": result.result.message.content[0].text
            })

        logger.info(f"âœ… Retrieved {len(results)} batch results (50% cost savings)")
        return results
```

### Phase 4: Hybrid Local + Cloud (Week 6)
**Goal:** Offload simple tasks to local LLM

```python
# src/hybrid_processor.py (NEW FILE)
class HybridProcessor:
    """Combine free local LLM with paid Claude API"""

    def __init__(self):
        self.local_model = LocalLLMHandler()  # Free Llama 3.1
        self.cloud_orchestrator = MultiAgentOrchestrator()  # Paid Claude

    def classify_query(self, query: str) -> str:
        """Use local model to classify complexity (FREE)"""

        prompt = f"""Classify this research query complexity:
        Query: {query}

        Options:
        - simple: Single fact lookup, basic Q&A
        - moderate: Multi-document analysis, comparison
        - complex: Deep research, cross-reference synthesis

        Classification:"""

        classification = self.local_model.generate(prompt, max_tokens=50)
        return classification.strip().lower()

    async def process(self, query: str, rag_system) -> Dict:
        """Route to local or cloud based on complexity"""

        # Step 1: Classify with local model (FREE)
        complexity = self.classify_query(query)
        logger.info(f"ðŸ“Š Query classified as: {complexity}")

        if complexity == "simple":
            # Step 2a: Answer with local model (FREE)
            logger.info("ðŸ†“ Using local LLM (free)")
            context, _ = rag_system.get_relevant_context(query, max_chunks=3)
            answer = self.local_model.generate(
                f"Answer this query: {query}\n\nContext:\n{context}",
                max_tokens=2000
            )
            return {"answer": answer, "cost": 0.0}

        elif complexity == "moderate":
            # Step 2b: Use single Sonnet agent (LOW COST)
            logger.info("âš¡ Using single Sonnet agent (low cost)")
            from src.comprehensive_analyzer import ComprehensiveAnalyzer
            analyzer = ComprehensiveAnalyzer()
            result = analyzer.create_comprehensive_summary(
                rag_system,
                documents_data=[],
                focus_areas=[query]
            )
            return {"result": result, "cost": 0.05}

        else:
            # Step 2c: Use full multi-agent system (HIGHER COST)
            logger.info("ðŸš€ Using multi-agent system (higher cost)")
            result = await self.cloud_orchestrator.research(query, rag_system)
            return {"result": result, "cost": 0.30}
```

## Cost Comparison: Before vs After

### Current Single-Agent System
```
Per Query Cost: $0.10 - 0.30
- Single Sonnet call: 5K tokens Ã— $3/1M = $0.015
- With caching: $0.005
- Simple queries overpay, complex queries underserve
```

### Multi-Agent WITHOUT Optimization
```
Per Query Cost: $0.50 - 5.00
- 1 Opus planning: $0.15
- 5 Opus workers: $0.60
- 1 Opus synthesis: $0.15
- Total: $0.90 per query (6Ã— more expensive)
```

### Multi-Agent WITH Full Optimization
```
Per Query Cost: $0.05 - 0.30
- Simple queries (70% of traffic):
  â†’ Local model (free) = $0.00

- Moderate queries (25% of traffic):
  â†’ 1 Sonnet agent = $0.015
  â†’ With caching = $0.005

- Complex queries (5% of traffic):
  â†’ 1 Opus planning = $0.045 (cached)
  â†’ 4 Sonnet workers = $0.096 (cached)
  â†’ 1 Opus synthesis = $0.045 (cached)
  â†’ Total = $0.186 per complex query

Weighted Average: (0.7 Ã— $0.00) + (0.25 Ã— $0.005) + (0.05 Ã— $0.186)
                = $0.00 + $0.00125 + $0.0093
                = $0.01 per query average

With batch mode for 50% of queries: $0.005 per query average
```

## Cost Monitoring Dashboard

Add real-time cost tracking to UI:

```python
# src/cost_tracker.py (NEW FILE)
class CostTracker:
    """Track API costs in real-time"""

    PRICING = {
        "claude-opus-4": {"input": 15.0, "output": 75.0},  # per 1M tokens
        "claude-sonnet-4": {"input": 3.0, "output": 15.0},
        "cache_read": 0.30,  # per 1M tokens (90% discount)
        "batch_discount": 0.5  # 50% off
    }

    def calculate_cost(self, usage: Dict) -> float:
        """Calculate cost from token usage"""

        model = usage.get("model")
        input_tokens = usage.get("input_tokens", 0)
        output_tokens = usage.get("output_tokens", 0)
        cache_read = usage.get("cache_read_input_tokens", 0)
        is_batch = usage.get("is_batch", False)

        # Calculate base cost
        input_cost = (input_tokens / 1_000_000) * self.PRICING[model]["input"]
        output_cost = (output_tokens / 1_000_000) * self.PRICING[model]["output"]
        cache_cost = (cache_read / 1_000_000) * self.PRICING["cache_read"]

        total_cost = input_cost + output_cost + cache_cost

        # Apply batch discount
        if is_batch:
            total_cost *= self.PRICING["batch_discount"]

        return total_cost

    def display_savings(self, usage_with_optimization: Dict, usage_without_optimization: Dict):
        """Show cost savings from optimization"""

        cost_with = self.calculate_cost(usage_with_optimization)
        cost_without = self.calculate_cost(usage_without_optimization)
        savings = cost_without - cost_with
        savings_pct = (savings / cost_without) * 100

        logger.info(f"ðŸ’° Cost: ${cost_with:.4f} (saved ${savings:.4f} / {savings_pct:.1f}%)")
```

## Recommended Settings for Cost-Conscious Users

```python
# config/settings.py

# Cost optimization flags
ENABLE_PROMPT_CACHING = True  # 60-70% savings
ENABLE_BATCH_MODE = True  # 50% savings for async
ENABLE_MODEL_ROUTING = True  # 40-60% savings
ENABLE_HYBRID_MODE = True  # Route simple queries to local model

# Token budgets by query complexity
TOKEN_BUDGET_SIMPLE = 2000  # Local model or single Sonnet
TOKEN_BUDGET_MODERATE = 10000  # Single Sonnet with caching
TOKEN_BUDGET_COMPLEX = 50000  # Multi-agent with Opus + Sonnet

# Multi-agent configuration
MAX_WORKER_AGENTS = 3  # Start conservative (instead of 5)
USE_OPUS_FOR_PLANNING = True  # Opus for planning, Sonnet for execution
USE_SONNET_FOR_WORKERS = True  # Always use cheaper model for workers

# Progressive research (start small, scale up if needed)
PROGRESSIVE_RESEARCH = True
START_WITH_SINGLE_AGENT = True  # Try 1 agent first
ESCALATE_TO_MULTI_AGENT = True  # Only use multi-agent if needed

# Batch processing (for background tasks)
BATCH_NON_URGENT_QUERIES = True
BATCH_CHECK_INTERVAL = 300  # Check every 5 minutes
```

## Implementation Priority

1. **Week 1**: Model routing (Opus + Sonnet) â†’ **40-60% cost reduction**
2. **Week 2**: Multi-agent with 3 workers (not 5) â†’ **Better quality, controlled cost**
3. **Week 3**: Batch mode for background tasks â†’ **50% reduction on batched queries**
4. **Week 4**: Hybrid local+cloud routing â†’ **Free processing for 70% of queries**
5. **Week 5**: Progressive research escalation â†’ **Start cheap, scale up only if needed**

## Expected Results

**Cost per query (optimized):**
- Simple queries (70%): $0.00 (local model)
- Moderate queries (25%): $0.005-0.015 (Sonnet + cache)
- Complex queries (5%): $0.15-0.30 (multi-agent Opus+Sonnet)

**Weighted average: $0.01 per query**

**Monthly cost estimates:**
- 1,000 queries/month: $10
- 10,000 queries/month: $100
- 100,000 queries/month: $1,000

**Quality improvement:**
- 10Ã— better retrieval (hybrid search + reranking)
- 3-5Ã— deeper analysis (multi-agent vs single-agent)
- Match Claude.ai research quality

## Next Steps

1. Implement `ModelRouter` class (route Opus/Sonnet)
2. Create `MultiAgentOrchestrator` with 3 worker agents
3. Add `HybridProcessor` for local/cloud routing
4. Enable batch mode in settings
5. Add cost tracking dashboard to UI

This approach gives you **Claude-quality research at 1/10th the cost** through intelligent optimization.
