# ===================================
# Research Assistant - Environment Configuration
# ===================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control!

# ===================================
# Model Selection
# ===================================
# Options: "api" for Claude API, "grok" for xAI Grok API, or "local" for Local LLM (Ollama)
MODEL_MODE=api

# ===================================
# Claude API Configuration (Required if MODEL_MODE=api)
# ===================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ===================================
# Grok API Configuration (Required if MODEL_MODE=grok)
# ===================================
# Get your API key from: https://x.ai/
# Use GROK_API_KEY for local development, XAI_API for Streamlit Cloud
GROK_API_KEY=your_grok_api_key_here
# Optional: Specify Grok model (default: grok-4-fast-reasoning)
GROK_MODEL=grok-4-fast-reasoning
GROK_MAX_TOKENS=8192
GROK_TEMPERATURE=0.7

# ===================================
# Tavily Web Search API (Optional - for enhanced web search)
# ===================================
# Get your API key from: https://tavily.com/
# Leave empty to disable web search feature
TAVILY_API_KEY=your_tavily_api_key_here

# Enable/disable web search functionality
ENABLE_WEB_SEARCH=true

# ===================================
# Local LLM Configuration (Required if MODEL_MODE=local)
# ===================================
# Ollama server URL (default: http://localhost:11434)
LOCAL_MODEL_URL=http://localhost:11434

# Model name (e.g., llama3.1:latest, deepseek-coder:latest, mistral:latest)
LOCAL_MODEL_NAME=llama3.1:latest

# Maximum tokens for local model generation
LOCAL_MODEL_MAX_TOKENS=8000

# Temperature for local model (0.0-1.0)
LOCAL_MODEL_TEMPERATURE=0.7

# Timeout in seconds for local model requests
LOCAL_MODEL_TIMEOUT=900

# Whether local model supports vision (true/false)
LOCAL_VISION_CAPABLE=false

# ===================================
# Application Limits
# ===================================
# Maximum file upload size in MB
MAX_UPLOAD_SIZE_MB=100

# Maximum number of documents to process at once
MAX_DOCUMENTS=10

# Processing timeout in minutes
PROCESSING_TIMEOUT_MINUTES=15

# ===================================
# Cost Optimization Settings (Claude API only)
# ===================================
# Enable prompt caching for 60-90% cost savings (recommended)
ENABLE_PROMPT_CACHING=true

# Enable batch mode for 50% cost savings (24h delivery time)
ENABLE_BATCH_MODE=false

# Batch status check interval in seconds
BATCH_CHECK_INTERVAL=60

# ===================================
# Multi-Agent System Settings (Claude API only)
# ===================================
# Enable multi-agent architecture for better quality
ENABLE_MULTI_AGENT=true

# Number of parallel worker agents (3-5 recommended)
NUM_WORKER_AGENTS=4

# Maximum chunks per agent for context
MULTI_AGENT_MAX_CHUNKS=10

# ===================================
# Performance Settings
# ===================================
# Show timing metrics in logs
ENABLE_TIMING_METRICS=true

# Show detailed progress in UI
SHOW_DETAILED_PROGRESS=true

# HuggingFace tokenizer parallelism (keep false to avoid warnings)
TOKENIZERS_PARALLELISM=false

# ===================================
# Logging
# ===================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ===================================
# Streamlit Cloud Deployment Notes
# ===================================
# When deploying to Streamlit Cloud:
# 1. Add your API keys in the Streamlit Cloud Secrets management
# 2. For Grok, use the secret name: XAI_API
#    For Claude, use: ANTHROPIC_API_KEY
#    For Tavily, use: TAVILY_API_KEY
# 3. Set MODEL_MODE to either "api" (Claude) or "grok" (xAI Grok)
# 4. Ensure all required packages are in requirements.txt
# 5. Local models (MODEL_MODE=local) are not supported on Streamlit Cloud
